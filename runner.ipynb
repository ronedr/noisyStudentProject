{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "runner.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "gea4bUFAcOUa"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xj3jiGd9bNrT"
      },
      "source": [
        "## All imports:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOR0jeSjRTne",
        "outputId": "c8033cf3-f75a-4423-fbb2-3e39288bddbb"
      },
      "source": [
        "!pip install scikit_posthocs"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit_posthocs in /usr/local/lib/python3.7/dist-packages (0.6.7)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.7/dist-packages (from scikit_posthocs) (0.10.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from scikit_posthocs) (1.19.5)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from scikit_posthocs) (0.11.1)\n",
            "Requirement already satisfied: pandas>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from scikit_posthocs) (1.1.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from scikit_posthocs) (3.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from scikit_posthocs) (1.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.20.0->scikit_posthocs) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.20.0->scikit_posthocs) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.20.0->scikit_posthocs) (1.15.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->scikit_posthocs) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->scikit_posthocs) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->scikit_posthocs) (0.10.0)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from statsmodels->scikit_posthocs) (0.5.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42Sdbd7_bK0_"
      },
      "source": [
        "import tensorflow as tf\n",
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "sess = tf.compat.v1.Session(config=config)\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import glob\n",
        "import tensorflow.keras as K\n",
        "from tensorflow.keras import layers\n",
        "import datetime\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n",
        "from tensorflow.keras.layers import Conv2DTranspose, Reshape, Lambda, Conv2D, MaxPool2D, Flatten, Dense, Dropout, BatchNormalization, Input, Activation, MaxPooling2D\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow.keras as K\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import time\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "from tqdm import tqdm\n",
        "from numpy import argmax\n",
        "import pandas as pd\n",
        "from scipy.stats import friedmanchisquare\n",
        "import scikit_posthocs"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qEVTAXDbT1D"
      },
      "source": [
        "## Constants:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-XAexVERaBI"
      },
      "source": [
        "Connection to drive - contains the datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvJQAcsQbp_7",
        "outputId": "dcd9dfb7-8d6b-447f-e981-c921d94cf7fb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aP6RDlr9bIhp"
      },
      "source": [
        "datasets_path_drive = '/content/drive/My Drive/NoisyStudentProject/Datasets/'\n",
        "path_drive = '/content/drive/My Drive/NoisyStudentProject/'"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPl3vLfPR4SS"
      },
      "source": [
        "### Load all datasets: \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ac7FlcQkblJc"
      },
      "source": [
        "def get_datasets():\n",
        "  datasets = []\n",
        "  datasets_name = set()\n",
        "  for file in glob.glob(datasets_path_drive+'*'):\n",
        "    datasets_name.add(os.path.splitext(file.split('/')[-1])[0][:-2])\n",
        "  for dataset_name in datasets_name:\n",
        "    data_X = datasets_path_drive + dataset_name + '_X.npy'\n",
        "    data_y = datasets_path_drive + dataset_name + '_Y.npy'\n",
        "    datasets += [((np.load(data_X), np.load(data_y)), dataset_name)]\n",
        "  return datasets"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39ZXfALkgoCk"
      },
      "source": [
        "## Classes of models:\n",
        " - BaseModelVGG16\n",
        " - ArticleModel\n",
        " - improvedArticleModel\n",
        "\n",
        "Each model contains 4 function: \n",
        "  build model - create the model\n",
        "  \n",
        "  train - complie the model and fit\n",
        "  \n",
        "  eval_acc - calculate the accuracy of model\n",
        "  \n",
        "  eval - calculate all matrics:\n",
        "\n",
        "    A. Accuracy – Under the assumption that the classification is the Class with the highest\n",
        "    probability.\n",
        "    B. TPR\n",
        "    C. FPR\n",
        "    D. Precision\n",
        "    E. AUC – Area Under the ROC Curve\n",
        "    F. Area under the Precision-Recall\n",
        "    G. Training time\n",
        "    H. Inference time for 1000 instances.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Kh5RoS1VJqv"
      },
      "source": [
        "### *BaseModelVGG16:*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NL7IhqFVCOr"
      },
      "source": [
        "class BaseModelVGG16:\n",
        "  def __init__(self, data, classes, batch_size, learning_rate, pooling, weight):\n",
        "    (self.x_train, self.y_train), (self.x_test, self.y_test) = data\n",
        "    self.input_shape = self.x_train.shape[1:]\n",
        "    self.weight = weight\n",
        "    self.pooling = pooling\n",
        "    self.bs = batch_size\n",
        "    self.lr = learning_rate\n",
        "    self.classes = classes\n",
        "    self.model = self.build_model()\n",
        "\n",
        "  def build_model(self):\n",
        "    base_model = VGG16(include_top=False, weights=self.weight, pooling=self.pooling, input_shape=self.input_shape)\n",
        "    model = Sequential()\n",
        "    model.add(base_model)\n",
        "    model.add(Dense(self.classes, activation='softmax'))\n",
        "    return model\n",
        "\n",
        "  def train(self, number_of_epochs):\n",
        "    self.model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=tf.keras.optimizers.Adam(learning_rate=self.lr), metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
        "    hist = self.model.fit(self.x_train, self.y_train, batch_size=self.bs, epochs=number_of_epochs, verbose=1)\n",
        "    return hist.history\n",
        "\n",
        "\n",
        "  def eval_acc(self):\n",
        "    results_dict = {}\n",
        "    prediction = self.model.predict(self.x_test)\n",
        "    soft = np.reshape(prediction, (self.y_test.shape[0], self.classes))\n",
        "    classes = np.unique(self.y_test)\n",
        "\n",
        "    # A. Accuracy\n",
        "    acc_eval = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "    acc_eval.update_state(self.y_test, soft)\n",
        "    acc = acc_eval.result().numpy()\n",
        "    results_dict['acc'] = acc\n",
        "    return acc\n",
        "\n",
        "  def eval(self):\n",
        "    '''\n",
        "    Evaluate and calculate all metrics according to the assignment requirements\n",
        "    :return: A dictionary contains all metrices\n",
        "    '''\n",
        "    results_dict = {}\n",
        "    prediction = self.model.predict(self.x_test)\n",
        "    soft = np.reshape(prediction, (self.y_test.shape[0], self.classes))\n",
        "    classes = np.unique(self.y_test)\n",
        "\n",
        "    # A. Accuracy\n",
        "    acc_eval = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "    acc_eval.update_state(self.y_test, soft)\n",
        "    acc = acc_eval.result().numpy()\n",
        "    print(acc)\n",
        "    results_dict['acc'] = acc\n",
        "\n",
        "    # B. TPR\n",
        "    pred_labels = soft.argmax(axis=1)\n",
        "    total_tp = 0\n",
        "    total_fp = 0\n",
        "    total_tn = 0\n",
        "    total_fn = 0\n",
        "    for label in classes:\n",
        "        for i in range(len(pred_labels)):\n",
        "            if self.y_test[i][0] == pred_labels[i] == label:\n",
        "                total_tp += 1\n",
        "\n",
        "            if pred_labels[i] == label and self.y_test[i][0] != label:\n",
        "                total_fp += 1\n",
        "\n",
        "            if pred_labels[i] != label and self.y_test[i][0] != label:\n",
        "                total_tn += 1\n",
        "\n",
        "            if pred_labels[i] != label and self.y_test[i][0] == label:\n",
        "                total_fn += 1\n",
        "\n",
        "    results_dict['TPR'] = total_tp / (total_tp + total_fn)\n",
        "\n",
        "    # C. FPR\n",
        "    results_dict['FPR'] = total_fp / (total_tn + total_fp)\n",
        "\n",
        "    # D. Precision\n",
        "    results_dict['Presicion'] = total_tp / (total_tp + total_fp)\n",
        "\n",
        "    # E. AUC – Area Under the ROC Curve\n",
        "    y_true = self.y_test.reshape((self.y_test.shape[0],))\n",
        "    y_pred = soft\n",
        "    results_dict['AUC'] = roc_auc_score(y_true, y_pred, 'macro', multi_class='ovr')\n",
        "    y_oh = tf.keras.utils.to_categorical(y_true)\n",
        "\n",
        "    # F. Area under the Precision-Recall\n",
        "    results_dict['Area under PR'] = average_precision_score(y_oh, y_pred, 'macro')\n",
        "\n",
        "    # H. Inference time for 1000 instances\n",
        "    if self.x_test.shape[0] < 1000:\n",
        "        inf_data = self.x_test\n",
        "    else:\n",
        "        inf_data = self.x_test[:1000]\n",
        "    start = time.time()\n",
        "    self.model.predict(inf_data)\n",
        "    end = time.time()\n",
        "    results_dict['Inferece time'] = end - start\n",
        "    return results_dict\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPY9wsZ8VPZi"
      },
      "source": [
        "### *ArticleModel:*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgD97iOHVF5P"
      },
      "source": [
        "class articleModel:\n",
        "  def __init__(self, data, X_unlabeled, classes, batch_size, learning_rate):\n",
        "    (self.x_train, self.y_train), (self.x_test, self.y_test) = data\n",
        "    self.X_unlabeled = X_unlabeled\n",
        "    self.input_shape = self.x_train.shape[1:]\n",
        "    self.bs = batch_size\n",
        "    self.lr = learning_rate\n",
        "    self.classes = classes\n",
        "    self.teacher = self.build_model()\n",
        "    self.student = self.build_model()\n",
        "    self.model = None\n",
        "\n",
        "  def build_model(self):\n",
        "    base_model = K.applications.EfficientNetB3(include_top=False, weights='imagenet', drop_connect_rate=0.4)\n",
        "    resize = K.Sequential([\n",
        "      K.layers.experimental.preprocessing.Resizing(self.input_shape[0], self.input_shape[1])\n",
        "    ])\n",
        "    model= K.Sequential()\n",
        "    model.add(resize)\n",
        "    model.add(base_model)\n",
        "    model.add(K.layers.Flatten())\n",
        "    model.add(K.layers.Dense(512, activation=('relu')))\n",
        "    model.add(K.layers.Dropout(0.2))\n",
        "    model.add(K.layers.Dense(256, activation=('relu')))\n",
        "    model.add(K.layers.Dropout(0.2))\n",
        "    model.add(K.layers.Dense(self.classes, activation=('softmax')))\n",
        "    return model\n",
        "\n",
        "\n",
        "  def train(self, number_of_epochs):\n",
        "    self.teacher.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    self.student.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    self.teacher.fit(x=self.x_train, y=self.y_train,\n",
        "        batch_size=self.bs,\n",
        "        epochs=number_of_epochs, shuffle=True,\n",
        "        verbose=1\n",
        "        )\n",
        "    x_train_student, y_train_student = pseudo_labelling(self.teacher, self.x_train, self.y_train, self.X_unlabeled, self.classes, threhold=0.0001)\n",
        "    batch_size = self.bs\n",
        "    steps_per_epoch = x_train_student.shape[0] // batch_size\n",
        "    self.student.fit(data_generator(x_train_student, y_train_student, batch_size, data_aug = True),\n",
        "                        epochs=number_of_epochs,\n",
        "                        steps_per_epoch = steps_per_epoch,\n",
        "                        batch_size=batch_size,\n",
        "                        verbose=1\n",
        "                        )\n",
        "    self.model = self.student\n",
        "\n",
        "\n",
        "  def eval_acc(self):\n",
        "    results_dict = {}\n",
        "    prediction = self.model.predict(self.x_test)\n",
        "    soft = np.reshape(prediction, (self.y_test.shape[0], self.classes))\n",
        "    classes = np.unique(self.y_test)\n",
        "\n",
        "    # A. Accuracy\n",
        "    acc_eval = tf.keras.metrics.CategoricalAccuracy()\n",
        "    acc_eval.update_state(self.y_test, soft)\n",
        "    acc = acc_eval.result().numpy()\n",
        "    results_dict['acc'] = acc\n",
        "    return acc\n",
        "\n",
        "  def eval(self):\n",
        "    '''\n",
        "    Evaluate and calculate all metrics according to the assignment requirements\n",
        "    :return: A dictionary contains all metrices\n",
        "    '''\n",
        "    results_dict = {}\n",
        "    prediction = self.model.predict(self.x_test)\n",
        "    soft = np.reshape(prediction, (self.y_test.shape[0], self.classes))\n",
        "    classes = np.unique(self.y_test)\n",
        "\n",
        "    # A. Accuracy\n",
        "    acc_eval = tf.keras.metrics.CategoricalAccuracy()\n",
        "    acc_eval.update_state(self.y_test, soft)\n",
        "    acc = acc_eval.result().numpy()\n",
        "    print(acc)\n",
        "    results_dict['acc'] = acc\n",
        "\n",
        "    # B. TPR\n",
        "    pred_labels = soft.argmax(axis=1)\n",
        "    total_tp = 0\n",
        "    total_fp = 0\n",
        "    total_tn = 0\n",
        "    total_fn = 0\n",
        "    for label in classes:\n",
        "        for i in range(len(pred_labels)):\n",
        "            if self.y_test[i][0] == pred_labels[i] == label:\n",
        "                total_tp += 1\n",
        "\n",
        "            if pred_labels[i] == label and self.y_test[i][0] != label:\n",
        "                total_fp += 1\n",
        "\n",
        "            if pred_labels[i] != label and self.y_test[i][0] != label:\n",
        "                total_tn += 1\n",
        "\n",
        "            if pred_labels[i] != label and self.y_test[i][0] == label:\n",
        "                total_fn += 1\n",
        "\n",
        "    results_dict['TPR'] = total_tp / (total_tp + total_fn)\n",
        "\n",
        "    # C. FPR\n",
        "    results_dict['FPR'] = total_fp / (total_tn + total_fp)\n",
        "\n",
        "    # D. Precision\n",
        "    results_dict['Presicion'] = total_tp / (total_tp + total_fp)\n",
        "\n",
        "    # E. AUC – Area Under the ROC Curve\n",
        "    y_true = self.y_test.reshape((self.y_test.shape[0],))\n",
        "    y_pred = soft\n",
        "    results_dict['AUC'] = roc_auc_score(y_true, y_pred, 'macro', multi_class='ovr')\n",
        "    y_oh = tf.keras.utils.to_categorical(y_true)\n",
        "\n",
        "    # F. Area under the Precision-Recall\n",
        "    results_dict['Area under PR'] = average_precision_score(y_oh, y_pred, 'macro')\n",
        "\n",
        "    # H. Inference time for 1000 instances\n",
        "    if self.x_test.shape[0] < 1000:\n",
        "        inf_data = self.x_test\n",
        "    else:\n",
        "        inf_data = self.x_test[:1000]\n",
        "    start = time.time()\n",
        "    self.model.predict(inf_data)\n",
        "    end = time.time()\n",
        "    results_dict['Inferece time'] = end - start\n",
        "    return results_dict"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auvmjwQCVUkN"
      },
      "source": [
        "### *improveArticleModel:*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ny6Hktggoaf"
      },
      "source": [
        "class improvedArticleModel:\n",
        "  def __init__(self, data, X_unlabeled, classes, batch_size, learning_rate):\n",
        "    (self.x_train, self.y_train), (self.x_test, self.y_test) = data\n",
        "    self.X_unlabeled = X_unlabeled\n",
        "    self.input_shape = self.x_train.shape[1:]\n",
        "    self.bs = batch_size\n",
        "    self.lr = learning_rate\n",
        "    self.classes = classes\n",
        "    self.teacher = self.build_model()\n",
        "    self.student = self.build_model()\n",
        "    self.student2 = self.build_model()\n",
        "    self.model = None\n",
        "\n",
        "  def build_model(self):\n",
        "    base_model = K.applications.EfficientNetB3(include_top=False, weights='imagenet', drop_connect_rate=0.4)\n",
        "    resize = K.Sequential([\n",
        "      K.layers.experimental.preprocessing.Resizing(self.input_shape[0], self.input_shape[1])\n",
        "    ])\n",
        "    model= K.Sequential()\n",
        "    model.add(resize)\n",
        "    model.add(base_model)\n",
        "    model.add(K.layers.Flatten())\n",
        "    model.add(K.layers.Dense(512, activation=('relu')))\n",
        "    model.add(K.layers.Dropout(0.2))\n",
        "    model.add(K.layers.Dense(256, activation=('relu')))\n",
        "    model.add(K.layers.Dropout(0.2))\n",
        "    model.add(K.layers.Dense(self.classes, activation=('softmax')))\n",
        "    return model\n",
        "\n",
        "\n",
        "  def train(self, number_of_epochs):\n",
        "    self.teacher.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    self.student.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    self.student2.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    batch_size = self.bs\n",
        "    self.teacher.fit(x=self.x_train, y=self.y_train,\n",
        "        batch_size=self.bs,\n",
        "        #validation_data=(Xv_p, Yv_p),\n",
        "        epochs=number_of_epochs, shuffle=True,\n",
        "        #callbacks=callback,\n",
        "        verbose=1\n",
        "        ) \n",
        "    x_train_student, y_train_student = pseudo_labelling(self.teacher, self.x_train, self.y_train, self.X_unlabeled[:(len(X_unlabeled)//2)], self.classes, threhold=0.0001)\n",
        "    #steps_per_epoch1 = (x_train_student.shape[0] // 2) // batch_size\n",
        "    steps_per_epoch1 = x_train_student.shape[0] // batch_size\n",
        "    self.student.fit(data_generator(x_train_student, y_train_student, batch_size, data_aug = True),\n",
        "                        epochs=number_of_epochs,\n",
        "                        steps_per_epoch = steps_per_epoch1,\n",
        "                        batch_size=batch_size,\n",
        "                        #validation_data = (Xv_p, Yv_p),\n",
        "                        #validation_steps = validation_steps,\n",
        "                        #callbacks=callback,\n",
        "                        verbose=1\n",
        "                        )\n",
        "    x_train_student2, y_train_student2 = pseudo_labelling(self.student, self.x_train, self.y_train, self.X_unlabeled[(len(X_unlabeled)//2):], self.classes, threhold=0.0001)\n",
        "    steps_per_epoch2 = x_train_student2.shape[0] // batch_size\n",
        "    self.student2.fit(data_generator(x_train_student2, y_train_student2, batch_size, data_aug = True),\n",
        "                        epochs=number_of_epochs,\n",
        "                        steps_per_epoch = steps_per_epoch2,\n",
        "                        batch_size=batch_size,\n",
        "                        #validation_data = (Xv_p, Yv_p),\n",
        "                        #validation_steps = validation_steps,\n",
        "                        #callbacks=callback,\n",
        "                        verbose=1\n",
        "                        )\n",
        "    self.model = self.student2\n",
        "\n",
        "\n",
        "  def eval_acc(self):\n",
        "    results_dict = {}\n",
        "    prediction = self.model.predict(self.x_test)\n",
        "    soft = np.reshape(prediction, (self.y_test.shape[0], self.classes))\n",
        "    classes = np.unique(self.y_test)\n",
        "\n",
        "    # A. Accuracy\n",
        "    acc_eval = tf.keras.metrics.CategoricalAccuracy()\n",
        "    acc_eval.update_state(self.y_test, soft)\n",
        "    acc = acc_eval.result().numpy()\n",
        "    results_dict['acc'] = acc\n",
        "    return acc\n",
        "\n",
        "  def eval(self):\n",
        "    '''\n",
        "    Evaluate and calculate all metrics according to the assignment requirements\n",
        "    :return: A dictionary contains all metrices\n",
        "    '''\n",
        "    results_dict = {}\n",
        "    prediction = self.model.predict(self.x_test)\n",
        "    soft = np.reshape(prediction, (self.y_test.shape[0], self.classes))\n",
        "    classes = np.unique(self.y_test)\n",
        "\n",
        "    # A. Accuracy\n",
        "    acc_eval = tf.keras.metrics.CategoricalAccuracy()\n",
        "    acc_eval.update_state(self.y_test, soft)\n",
        "    acc = acc_eval.result().numpy()\n",
        "    print(acc)\n",
        "    results_dict['acc'] = acc\n",
        "\n",
        "    # B. TPR\n",
        "    pred_labels = soft.argmax(axis=1)\n",
        "    total_tp = 0\n",
        "    total_fp = 0\n",
        "    total_tn = 0\n",
        "    total_fn = 0\n",
        "    for label in classes:\n",
        "        for i in range(len(pred_labels)):\n",
        "            if self.y_test[i][0] == pred_labels[i] == label:\n",
        "                total_tp += 1\n",
        "\n",
        "            if pred_labels[i] == label and self.y_test[i][0] != label:\n",
        "                total_fp += 1\n",
        "\n",
        "            if pred_labels[i] != label and self.y_test[i][0] != label:\n",
        "                total_tn += 1\n",
        "\n",
        "            if pred_labels[i] != label and self.y_test[i][0] == label:\n",
        "                total_fn += 1\n",
        "\n",
        "    results_dict['TPR'] = total_tp / (total_tp + total_fn)\n",
        "\n",
        "    # C. FPR\n",
        "    results_dict['FPR'] = total_fp / (total_tn + total_fp)\n",
        "\n",
        "    # D. Precision\n",
        "    results_dict['Presicion'] = total_tp / (total_tp + total_fp)\n",
        "\n",
        "    # E. AUC – Area Under the ROC Curve\n",
        "    y_true = self.y_test.reshape((self.y_test.shape[0],))\n",
        "    y_pred = soft\n",
        "    results_dict['AUC'] = roc_auc_score(y_true, y_pred, 'macro', multi_class='ovr')\n",
        "    y_oh = tf.keras.utils.to_categorical(y_true)\n",
        "\n",
        "    # F. Area under the Precision-Recall\n",
        "    results_dict['Area under PR'] = average_precision_score(y_oh, y_pred, 'macro')\n",
        "\n",
        "    # H. Inference time for 1000 instances\n",
        "    if self.x_test.shape[0] < 1000:\n",
        "        inf_data = self.x_test\n",
        "    else:\n",
        "        inf_data = self.x_test[:1000]\n",
        "    start = time.time()\n",
        "    self.model.predict(inf_data)\n",
        "    end = time.time()\n",
        "    results_dict['Inferece time'] = end - start\n",
        "    return results_dict\n"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eavdl_L3gJ21"
      },
      "source": [
        "## Train functions:\n",
        "As required, we used external 10-fold cross validation, in addition to an internal 3-fold cross validation for hyperparameter optimization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIvAtFkRgC8P"
      },
      "source": [
        "def train_baseline_model(dataset_name, data, outer_cv=10, inner_cv=3, random_search_trials=25, inner_epochs=1, outer_epochs=5):\n",
        "  X, y = data\n",
        "  outer_skf = StratifiedKFold(n_splits=outer_cv, random_state=7, shuffle=True)\n",
        "  inner_skf = StratifiedKFold(n_splits=inner_cv, random_state=7, shuffle=True)\n",
        "  fold_var = 0\n",
        "  list_of_res = []\n",
        "  for train_index, val_index in outer_skf.split(X=X, y=y):\n",
        "    X_train = X[train_index]\n",
        "    y_train = y[train_index]\n",
        "    X_val = X[val_index]\n",
        "    y_val = y[val_index]\n",
        "    hyper_param_batchsize = np.array([64, 128])\n",
        "    hyper_param_lr = np.array([0.005, 0.001, 0.0001])\n",
        "    hyper_param_pooling = np.array(['max', 'avg'])\n",
        "    hyper_param_weights = np.array(['imagenet', None])\n",
        "    hyper_params_dict = {}\n",
        "    max_trail = 0\n",
        "    max_acc = 0\n",
        "    for trail in range(random_search_trials):\n",
        "      batch_size_h = np.random.choice(hyper_param_batchsize)\n",
        "      learning_rate_h = np.random.choice(hyper_param_lr)\n",
        "      pooling_h = np.random.choice(hyper_param_pooling)\n",
        "      weights_h = np.random.choice(hyper_param_weights)\n",
        "      print(f\"hyper params {(batch_size_h, learning_rate_h, pooling_h, weights_h)}\")\n",
        "      acc_list = []\n",
        "      for train_index_inner, val_index_inner in inner_skf.split(X=X_train, y=y_train):\n",
        "        X_train_inner = X_train[train_index_inner]\n",
        "        y_train_inner = y_train[train_index_inner]\n",
        "        X_val_inner = X_train[val_index_inner]\n",
        "        y_val_inner = y_train[val_index_inner]\n",
        "        classes = len(np.unique(y_train_inner))\n",
        "        model = BaseModelVGG16(((X_train_inner,y_train_inner), (X_val_inner, y_val_inner)), classes, batch_size_h, learning_rate_h, pooling_h, weights_h)\n",
        "        model.train(inner_epochs)\n",
        "        acc = model.eval_acc()\n",
        "        acc_list.append(acc)\n",
        "        tf.keras.backend.clear_session()\n",
        "      mean_acc = np.array(acc_list).mean()\n",
        "      if mean_acc > max_acc:\n",
        "          max_trail = trail\n",
        "          max_acc = mean_acc\n",
        "      hyper_params_dict[trail] = (batch_size_h, learning_rate_h, pooling_h, weights_h, mean_acc)\n",
        "    \n",
        "    best_params = hyper_params_dict[max_trail]\n",
        "    model = BaseModelVGG16(((X_train, y_train), (X_val, y_val)), classes, best_params[0], best_params[1], best_params[2], best_params[3])\n",
        "    start_timer = time.time()\n",
        "    model.train(outer_epochs)\n",
        "    end_timer = time.time()\n",
        "    eval_res = model.eval()\n",
        "    results_dict = {}\n",
        "    results_dict['dataset_name'] = dataset_name\n",
        "    results_dict['k-fold'] = fold_var\n",
        "    results_dict['train_time'] = end_timer - start_timer\n",
        "    results_dict['hyper-parameters'] = f'batch_size={best_params[0]}, learning_rate={best_params[1]}, best_pooling={best_params[2]}, best_init_weights={best_params[3]}'\n",
        "    results_dict.update(eval_res)\n",
        "    list_of_res.append(results_dict)\n",
        "    tf.keras.backend.clear_session()\n",
        "    fold_var += 1\n",
        "    tmp = pd.DataFrame(list_of_res)\n",
        "    tmp.to_csv(f'{path_drive}Results/Baseline_{dataset_name}.csv', index=None)\n",
        "  return pd.DataFrame(list_of_res)\n",
        "\n",
        "\n",
        "def train_article_model(dataset_name, data, X_unlabeled, outer_cv=10, inner_cv=3, random_search_trials=25, inner_epochs=1, outer_epochs=5):\n",
        "  X, y = data\n",
        "  outer_skf = StratifiedKFold(n_splits=outer_cv, random_state=7, shuffle=True)\n",
        "  inner_skf = StratifiedKFold(n_splits=inner_cv, random_state=7, shuffle=True)\n",
        "  fold_var = 0\n",
        "  list_of_res = []\n",
        "  for train_index, val_index in outer_skf.split(X=X, y=y):\n",
        "    X_train = X[train_index]\n",
        "    y_train = y[train_index]\n",
        "    X_val = X[val_index]\n",
        "    y_val = y[val_index]\n",
        "    hyper_param_batchsize = np.array([64, 128])\n",
        "    hyper_param_lr = np.array([0.005, 0.001, 0.0001])\n",
        "    #hyper_param_pooling = np.array(['max', 'avg'])\n",
        "    #hyper_param_weights = np.array(['imagenet', None])\n",
        "    hyper_params_dict = {}\n",
        "    max_trail = 0\n",
        "    max_acc = 0\n",
        "    for trail in range(random_search_trials):\n",
        "      batch_size_h = np.random.choice(hyper_param_batchsize)\n",
        "      learning_rate_h = np.random.choice(hyper_param_lr)\n",
        "      #pooling_h = np.random.choice(hyper_param_pooling)\n",
        "      #weights_h = np.random.choice(hyper_param_weights)\n",
        "      print(f\"hyper params {(batch_size_h, learning_rate_h)}\")\n",
        "      acc_list = []\n",
        "      for train_index_inner, val_index_inner in inner_skf.split(X=X_train, y=y_train):\n",
        "        X_train_inner = X_train[train_index_inner]\n",
        "        y_train_inner = y_train[train_index_inner]\n",
        "        X_val_inner = X_train[val_index_inner]\n",
        "        y_val_inner = y_train[val_index_inner]\n",
        "        classes = len(np.unique(y_train_inner))\n",
        "        #print(\"one\")\n",
        "        model = articleModel(((X_train_inner,K.utils.to_categorical(y_train_inner, classes)), (X_val_inner, y_val_inner)), X_unlabeled, classes, batch_size_h, learning_rate_h)\n",
        "        #print(\"two\")\n",
        "        model.train(inner_epochs)\n",
        "        #print(\"three\")\n",
        "        acc = model.eval_acc()\n",
        "        acc_list.append(acc)\n",
        "        tf.keras.backend.clear_session()\n",
        "      mean_acc = np.array(acc_list).mean()\n",
        "      if mean_acc > max_acc:\n",
        "          max_trail = trail\n",
        "          max_acc = mean_acc\n",
        "      hyper_params_dict[trail] = (batch_size_h, learning_rate_h, mean_acc)\n",
        "    \n",
        "    best_params = hyper_params_dict[max_trail]\n",
        "    model = articleModel(((X_train_inner,K.utils.to_categorical(y_train_inner, classes)), (X_val_inner, y_val_inner)), X_unlabeled, classes, best_params[0], best_params[1])\n",
        "    start_timer = time.time()\n",
        "    model.train(outer_epochs)\n",
        "    end_timer = time.time()\n",
        "    eval_res = model.eval()\n",
        "    results_dict = {}\n",
        "    results_dict['dataset_name'] = dataset_name\n",
        "    results_dict['k-fold'] = fold_var\n",
        "    results_dict['train_time'] = end_timer - start_timer\n",
        "    results_dict['hyper-parameters'] = f'batch_size={best_params[0]}, learning_rate={best_params[1]}'\n",
        "    results_dict.update(eval_res)\n",
        "    list_of_res.append(results_dict)\n",
        "    tf.keras.backend.clear_session()\n",
        "    fold_var += 1\n",
        "    tmp = pd.DataFrame(list_of_res)\n",
        "    tmp.to_csv(f'{path_drive}Results/Article_{dataset_name}.csv', index=None)\n",
        "  return pd.DataFrame(list_of_res)\n",
        "\n",
        "\n",
        "def train_improve_model(dataset_name, data, X_unlabeled, outer_cv=10, inner_cv=3, random_search_trials=25, inner_epochs=1, outer_epochs=5):\n",
        "  X, y = data\n",
        "  outer_skf = StratifiedKFold(n_splits=outer_cv, random_state=7, shuffle=True)\n",
        "  inner_skf = StratifiedKFold(n_splits=inner_cv, random_state=7, shuffle=True)\n",
        "  fold_var = 0\n",
        "  list_of_res = []\n",
        "  for train_index, val_index in outer_skf.split(X=X, y=y):\n",
        "    X_train = X[train_index]\n",
        "    y_train = y[train_index]\n",
        "    X_val = X[val_index]\n",
        "    y_val = y[val_index]\n",
        "    hyper_param_batchsize = np.array([64, 128])\n",
        "    hyper_param_lr = np.array([0.005, 0.001, 0.0001])\n",
        "    #hyper_param_pooling = np.array(['max', 'avg'])\n",
        "    #hyper_param_weights = np.array(['imagenet', None])\n",
        "    hyper_params_dict = {}\n",
        "    max_trail = 0\n",
        "    max_acc = 0\n",
        "    for trail in range(random_search_trials):\n",
        "      batch_size_h = np.random.choice(hyper_param_batchsize)\n",
        "      learning_rate_h = np.random.choice(hyper_param_lr)\n",
        "      #pooling_h = np.random.choice(hyper_param_pooling)\n",
        "      #weights_h = np.random.choice(hyper_param_weights)\n",
        "      print(f\"hyper params {(batch_size_h, learning_rate_h)}\")\n",
        "      acc_list = []\n",
        "      for train_index_inner, val_index_inner in inner_skf.split(X=X_train, y=y_train):\n",
        "        X_train_inner = X_train[train_index_inner]\n",
        "        y_train_inner = y_train[train_index_inner]\n",
        "        X_val_inner = X_train[val_index_inner]\n",
        "        y_val_inner = y_train[val_index_inner]\n",
        "        classes = len(np.unique(y_train_inner))\n",
        "        model = improvedArticleModel(((X_train_inner,K.utils.to_categorical(y_train_inner, classes)), (X_val_inner, y_val_inner)), X_unlabeled, classes, batch_size_h, learning_rate_h)\n",
        "        model.train(inner_epochs)\n",
        "        acc = model.eval_acc()\n",
        "        acc_list.append(acc)\n",
        "        tf.keras.backend.clear_session()\n",
        "      mean_acc = np.array(acc_list).mean()\n",
        "      if mean_acc > max_acc:\n",
        "          max_trail = trail\n",
        "          max_acc = mean_acc\n",
        "      hyper_params_dict[trail] = (batch_size_h, learning_rate_h, mean_acc)\n",
        "    \n",
        "    best_params = hyper_params_dict[max_trail]\n",
        "    model = improvedArticleModel(((X_train_inner,K.utils.to_categorical(y_train_inner, classes)), (X_val_inner, y_val_inner)), X_unlabeled, classes, best_params[0], best_params[1])\n",
        "    start_timer = time.time()\n",
        "    model.train(outer_epochs)\n",
        "    end_timer = time.time()\n",
        "    eval_res = model.eval()\n",
        "    results_dict = {}\n",
        "    results_dict['dataset_name'] = dataset_name\n",
        "    results_dict['k-fold'] = fold_var\n",
        "    results_dict['train_time'] = end_timer - start_timer\n",
        "    results_dict['hyper-parameters'] = f'batch_size={best_params[0]}, learning_rate={best_params[1]}'\n",
        "    results_dict.update(eval_res)\n",
        "    list_of_res.append(results_dict)\n",
        "    tf.keras.backend.clear_session()\n",
        "    fold_var += 1\n",
        "    tmp = pd.DataFrame(list_of_res)\n",
        "    tmp.to_csv(f'{path_drive}Results/Improved_{dataset_name}.csv', index=None)\n",
        "  return pd.DataFrame(list_of_res)\n",
        "  "
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWiqDc1TT27l"
      },
      "source": [
        " #### RandAugment used in the algorithm teacher-student for robustness:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfNNdk8hDAUo"
      },
      "source": [
        "# Reference: code taken from: https://github.com/heartInsert/randaugment/blob/master/Rand_Augment.py\n",
        "# Rand Augmentation\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image, ImageEnhance, ImageOps\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class Rand_Augment():\n",
        "    def __init__(self, Numbers=None, max_Magnitude=None):\n",
        "        self.transforms = ['autocontrast', 'equalize', 'rotate', 'solarize', 'color', 'posterize',\n",
        "                           'contrast', 'brightness', 'sharpness', 'shearX', 'shearY', 'translateX', 'translateY']\n",
        "        if Numbers is None:\n",
        "            self.Numbers = len(self.transforms) // 2\n",
        "        else:\n",
        "            self.Numbers = Numbers\n",
        "        if max_Magnitude is None:\n",
        "            self.max_Magnitude = 10\n",
        "        else:\n",
        "            self.max_Magnitude = max_Magnitude\n",
        "        fillcolor = 128\n",
        "        self.ranges = {\n",
        "            # these  Magnitude   range , you  must test  it  yourself , see  what  will happen  after these  operation ,\n",
        "            # it is no  need to obey  the value  in  autoaugment.py\n",
        "            \"shearX\": np.linspace(0, 0.3, 10),\n",
        "            \"shearY\": np.linspace(0, 0.3, 10),\n",
        "            \"translateX\": np.linspace(0, 0.2, 10),\n",
        "            \"translateY\": np.linspace(0, 0.2, 10),\n",
        "            \"rotate\": np.linspace(0, 360, 10),\n",
        "            \"color\": np.linspace(0.0, 0.9, 10),\n",
        "            \"posterize\": np.round(np.linspace(8, 4, 10), 0).astype(np.int),\n",
        "            \"solarize\": np.linspace(256, 231, 10),\n",
        "            \"contrast\": np.linspace(0.0, 0.5, 10),\n",
        "            \"sharpness\": np.linspace(0.0, 0.9, 10),\n",
        "            \"brightness\": np.linspace(0.0, 0.3, 10),\n",
        "            \"autocontrast\": [0] * 10,\n",
        "            \"equalize\": [0] * 10,           \n",
        "            \"invert\": [0] * 10\n",
        "        }\n",
        "        self.func = {\n",
        "            \"shearX\": lambda img, magnitude: img.transform(\n",
        "                img.size, Image.AFFINE, (1, magnitude * random.choice([-1, 1]), 0, 0, 1, 0),\n",
        "                Image.BICUBIC, fill=fillcolor),\n",
        "            \"shearY\": lambda img, magnitude: img.transform(\n",
        "                img.size, Image.AFFINE, (1, 0, 0, magnitude * random.choice([-1, 1]), 1, 0),\n",
        "                Image.BICUBIC, fill=fillcolor),\n",
        "            \"translateX\": lambda img, magnitude: img.transform(\n",
        "                img.size, Image.AFFINE, (1, 0, magnitude * img.size[0] * random.choice([-1, 1]), 0, 1, 0),\n",
        "                fill=fillcolor),\n",
        "            \"translateY\": lambda img, magnitude: img.transform(\n",
        "                img.size, Image.AFFINE, (1, 0, 0, 0, 1, magnitude * img.size[1] * random.choice([-1, 1])),\n",
        "                fill=fillcolor),\n",
        "            \"rotate\": lambda img, magnitude: self.rotate_with_fill(img, magnitude),\n",
        "            # \"rotate\": lambda img, magnitude: img.rotate(magnitude * random.choice([-1, 1])),\n",
        "            \"color\": lambda img, magnitude: ImageEnhance.Color(img).enhance(1 + magnitude * random.choice([-1, 1])),\n",
        "            \"posterize\": lambda img, magnitude: ImageOps.posterize(img, magnitude),\n",
        "            \"solarize\": lambda img, magnitude: ImageOps.solarize(img, magnitude),\n",
        "            \"contrast\": lambda img, magnitude: ImageEnhance.Contrast(img).enhance(\n",
        "                1 + magnitude * random.choice([-1, 1])),\n",
        "            \"sharpness\": lambda img, magnitude: ImageEnhance.Sharpness(img).enhance(\n",
        "                1 + magnitude * random.choice([-1, 1])),\n",
        "            \"brightness\": lambda img, magnitude: ImageEnhance.Brightness(img).enhance(\n",
        "                1 + magnitude * random.choice([-1, 1])),\n",
        "            \"autocontrast\": lambda img, magnitude: ImageOps.autocontrast(img),\n",
        "            \"equalize\": lambda img, magnitude: img,\n",
        "            \"invert\": lambda img, magnitude: ImageOps.invert(img)\n",
        "        }\n",
        "\n",
        "    def rand_augment(self):\n",
        "        \"\"\"Generate a set of distortions.\n",
        "             Args:\n",
        "             N: Number of augmentation transformations to apply sequentially. N  is len(transforms)/2  will be best\n",
        "             M: Max_Magnitude for all the transformations. should be  <= self.max_Magnitude \"\"\"\n",
        "\n",
        "        M = np.random.randint(0, self.max_Magnitude, self.Numbers)\n",
        "\n",
        "        sampled_ops = np.random.choice(self.transforms, self.Numbers)\n",
        "        return [(op, Magnitude) for (op, Magnitude) in zip(sampled_ops, M)]\n",
        "\n",
        "    def __call__(self, image):\n",
        "        operations = self.rand_augment()\n",
        "        for (op_name, M) in operations:\n",
        "            operation = self.func[op_name]\n",
        "            mag = self.ranges[op_name][M]\n",
        "            image = operation(image, mag)\n",
        "        return image\n",
        "\n",
        "    def rotate_with_fill(self, img, magnitude):\n",
        "        #  I  don't know why  rotate  must change to RGBA , it is  copy  from Autoaugment - pytorch\n",
        "        rot = img.convert(\"RGBA\").rotate(magnitude)\n",
        "        return Image.composite(rot, Image.new(\"RGBA\", rot.size, (128,) * 4), rot).convert(img.mode)\n",
        "\n",
        "    def test_single_operation(self, image, op_name, M=-1):\n",
        "        '''\n",
        "        :param image: image\n",
        "        :param op_name: operation name in   self.transforms\n",
        "        :param M: -1  stands  for the  max   Magnitude  in  there operation\n",
        "        :return:\n",
        "        '''\n",
        "        operation = self.func[op_name]\n",
        "        mag = self.ranges[op_name][M]\n",
        "        image = operation(image, mag)\n",
        "        return image"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrgAQ3AIYH5L"
      },
      "source": [
        "### Predict lable by the trained model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gim1AF6VC3a8"
      },
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import subprocess as sp\n",
        "import os\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "img_augment = Rand_Augment(Numbers=2, max_Magnitude=10)\n",
        "\n",
        "def pseudo_labelling(model, xs, ys, xt, classes, threhold=0.0001):\n",
        "  \"\"\"\n",
        "  Pseudo-label unlabeled data in the teacher model\n",
        "      First, prepare an image for attaching a pseudo label. As a detailed procedure Make unlabeled images\n",
        "      into numpy arrays Add a pseudo label to an unlabeled image Leave only pseudo-label data above a certain \n",
        "      threshold Align the number of data for each label It will be. \n",
        "  \"\"\"\n",
        "  x_train_9,x_test_9, y_train_9,y_test_9 = train_test_split(xs, ys, test_size=0.2)\n",
        "\n",
        "  #y_train_9 = to_categorical(y_train_9)\n",
        "  #y_test_9 = to_categorical(y_test_9)\n",
        "\n",
        "  # ============Add a pseudo label to an unlabeled image============\n",
        "\n",
        "  x_train_imgnet = xt[:-1]\n",
        "  #Batch size setting\n",
        "  largest_divisor = 1\n",
        "  for i in range(2, len(x_train_imgnet)):\n",
        "    if len(x_train_imgnet) % i == 0:\n",
        "        largest_divisor = i \n",
        "  batch_size = largest_divisor\n",
        "  #How many steps\n",
        "  step = int(len(x_train_imgnet)/batch_size)\n",
        "\n",
        "  #Empty list for pseudo labels\n",
        "  y_train_imgnet_dummy = []\n",
        "\n",
        "  for i in range(step):\n",
        "      # Extract image data for batch size\n",
        "      x_temp = x_train_imgnet[batch_size*i:batch_size*(i+1)]\n",
        "      #normalization\n",
        "      x_temp = x_temp\n",
        "      #inference\n",
        "      temp = model.predict(x_temp)\n",
        "      #Add to empty list\n",
        "      y_train_imgnet_dummy.extend(temp)\n",
        "\n",
        "  #List to numpy array\n",
        "  y_train_imgnet_dummy = np.array(y_train_imgnet_dummy)\n",
        "  #print(\"y_train_imgnet_dummy is \" + str(y_train_imgnet_dummy))\n",
        "  # ============Leave only pseudo-label data above a certain threshold============\n",
        "  #Thresholding\n",
        "  y_train_imgnet_dummy_th =  y_train_imgnet_dummy[np.max(y_train_imgnet_dummy, axis=1) > threhold]\n",
        "  #holder1 = y_train_imgnet_dummy[np.max(y_train_imgnet_dummy, axis=1)]\n",
        "  #holder2 = np.max(y_train_imgnet_dummy, axis=1)\n",
        "  x_train_imgnet_th = x_train_imgnet[np.max(y_train_imgnet_dummy, axis=1) > threhold]\n",
        "  #print(\"y_train_imgnet_dummy_th is \" + str(y_train_imgnet_dummy_th))\n",
        "  #print(\"holder1 is \" + str(holder1))\n",
        "  #print(\"holder2 is \" + str(holder2))\n",
        "  #from onehot vector to class index\n",
        "  y_student_all_dummy_label = np.argmax(y_train_imgnet_dummy_th, axis=1)\n",
        "  #print(\"y_student_all_dummy_label is \" + str(y_student_all_dummy_label))\n",
        "  #Count the number of each class of pseudo-labels\n",
        "  u, counts = np.unique(y_student_all_dummy_label, return_counts=True)\n",
        "\n",
        "  #Calculate the maximum number of counts\n",
        "  student_label_max =  max(counts)\n",
        "\n",
        "  #Separate numpy array for each label\n",
        "  y_student_per_label = []\n",
        "  y_student_per_img_path = []\n",
        "\n",
        "  for i in range(classes):\n",
        "      temp_l = y_train_imgnet_dummy_th[y_student_all_dummy_label == i]\n",
        "      y_student_per_label.append(temp_l)\n",
        "      temp_i = x_train_imgnet_th[y_student_all_dummy_label == i]\n",
        "      y_student_per_img_path.append(temp_i)\n",
        "\n",
        "  #Copy data for maximum count on each label\n",
        "  y_student_per_label_add = []\n",
        "  y_student_per_img_add = []\n",
        "\n",
        "  for i in range(classes):\n",
        "      num = len(y_student_per_label[i])\n",
        "      temp_l = y_student_per_label[i]\n",
        "      temp_i = y_student_per_img_path[i]\n",
        "      add_num = student_label_max - num\n",
        "      q, mod = divmod(add_num, num)\n",
        "      temp_l_tile = np.tile(temp_l, (q+1, 1))\n",
        "      temp_i_tile = np.tile(temp_i, (q+1, 1, 1, 1))\n",
        "      temp_l_add = temp_l[:mod]\n",
        "      temp_i_add = temp_i[:mod]\n",
        "      y_student_per_label_add.append(np.concatenate([temp_l_tile, temp_l_add], axis=0))\n",
        "      y_student_per_img_add.append(np.concatenate([temp_i_tile, temp_i_add], axis=0))\n",
        "\n",
        "  #Check the count number of each label\n",
        "  #print([len(i) for i in y_student_per_label_add])\n",
        "\n",
        "  #Merge data for each label\n",
        "  student_train_img = np.concatenate(y_student_per_img_add, axis=0)\n",
        "  student_train_label = np.concatenate(y_student_per_label_add, axis=0)\n",
        "\n",
        "  # Combined with the original training data numpy array\n",
        "  x_train_student = np.concatenate([x_train_9, student_train_img], axis=0)\n",
        "  y_train_student = np.concatenate([y_train_9, student_train_label], axis=0)\n",
        "\n",
        "  return x_train_student, y_train_student\n",
        "\n",
        "\n",
        "# Data generator definition\n",
        "def get_random_data(x_train_i, y_train_i, data_aug):\n",
        "  x = tf.keras.preprocessing.image.array_to_img(x_train_i)\n",
        "\n",
        "  if data_aug:\n",
        "      seed_image = img_augment(x)\n",
        "      seed_image = tf.keras.preprocessing.image.img_to_array(seed_image)\n",
        "\n",
        "  else:\n",
        "      seed_image = x_train_i\n",
        "\n",
        "  seed_image = seed_image\n",
        "\n",
        "  return seed_image, y_train_i\n",
        "\n",
        "def data_generator(x_train, y_train, batch_size, data_aug):\n",
        "  n = len(x_train)\n",
        "  i = 0\n",
        "  while True:\n",
        "      image_data = []\n",
        "      label_data = []\n",
        "      for b in range(batch_size):\n",
        "          if i==0:\n",
        "              p = np.random.permutation(len(x_train))\n",
        "              x_train = x_train[p]\n",
        "              y_train = y_train[p]\n",
        "          image, label = get_random_data(x_train[i], y_train[i], data_aug)\n",
        "          image_data.append(image)\n",
        "          label_data.append(label)\n",
        "          i = (i+1) % n\n",
        "      image_data = np.array(image_data)\n",
        "      label_data = np.array(label_data)\n",
        "      yield image_data, label_data"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tShbryF8Vucp"
      },
      "source": [
        "### Train models delegation function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slyBoyWhVtiZ"
      },
      "source": [
        "def train(model_name, dataset_name, data, X_unlabeled):\n",
        "  if model_name == 'baseModel':\n",
        "    return train_baseline_model(dataset_name, data) # implement !!\n",
        "  elif model_name == 'articleModel':\n",
        "    return train_article_model(dataset_name, data, X_unlabeled) # need to implement !! \n",
        "  elif model_name == 'improveModel':\n",
        "    return train_improve_model(dataset_name, data, X_unlabeled) # need to implement !! "
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeQbTBnpUxDD"
      },
      "source": [
        "## 1. Start the experiment:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eqhVc9fU2ze"
      },
      "source": [
        "Load the datasets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtsKZ7JBOX9V"
      },
      "source": [
        "datasets = get_datasets()"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RrBzdwX0Jx9",
        "outputId": "953be0b4-c2aa-483b-f016-5639685a6bc1"
      },
      "source": [
        "print('names of the datasets:', [name for (i, name) in datasets])"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "names of the datasets: ['svhn_3', 'Cmater', 'svhn_2', 'Beans', 'svhn_1', 'Cifar100_5', 'Coloret', 'comp_2', 'stl_2', 'Oxford_2', 'comp_3', 'stl_3', 'Rps', 'Cifar100_2', 'Oxford_1', 'Cifar100_1', 'stl_1', 'comp_1', 'Cifar100_4', 'Cifar100_3']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOfOTOwAbvGt"
      },
      "source": [
        "# loading data and using preprocess for training and validation dataset\n",
        "results = []\n",
        "models = ['baseModel', 'articleModel', 'improveModel']\n",
        "for data in datasets:\n",
        "  for model_name in models:\n",
        "    dataset_data = data[0]\n",
        "    dataset_name = data[1]\n",
        "    X_data, y_data = dataset_data[0], dataset_data[1]\n",
        "    # Extract 20% for unlabeled data\n",
        "    X_train_data, X_unlabeled, y_train_data, _ = train_test_split(X_data, y_data, test_size=0.2, stratify=y_data) # X_test - unlableded data\n",
        "    # split the other data to train and test\n",
        "    print('---------', 'Train model:', model_name, ', on dataset:', dataset_name, ', number of classes:', len(np.unique(y_train_data)), ', train data size:', len(X_train_data), '---------')\n",
        "    model = train(model_name, dataset_name, (X_train_data, y_train_data), X_unlabeled)\n",
        "    results.append(model)\n",
        "results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50JwhL5IWvwd"
      },
      "source": [
        "### Statistical significance testing of the results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZE5Nv_VxCaAm"
      },
      "source": [
        "def friedman_test(excel_name):\n",
        "    \"\"\"\n",
        "    Collects the AUC for each algorithm and calculate friedman test. Returns the samples that ran through the test and\n",
        "    statistic and p-val of the test\n",
        "    \"\"\"\n",
        "    res = pd.read_excel(excel_name, sheet_name=None)\n",
        "    baseline_auc = []\n",
        "    paper_auc = []\n",
        "    improve_auc = []\n",
        "    for df_name, df in res.items():\n",
        "        baseline_auc.append(df[df['Model'] == 'Baseline']['AUC'].mean())\n",
        "        paper_auc.append(df[df['Model'] == 'Article']['AUC'].mean())\n",
        "        improve_auc.append(df[df['Model'] == 'Improved']['AUC'].mean())\n",
        "    x = friedmanchisquare(baseline_auc, paper_auc, improve_auc)\n",
        "    return baseline_auc, paper_auc, improve_auc, x\n",
        "\n",
        "\n",
        "def post_hoc_test(baseline_auc, paper_auc, improve_auc):\n",
        "    \"\"\"\n",
        "    Run posthoc_nemenyi test and print the results\n",
        "    \"\"\"\n",
        "    data = np.array([baseline_auc, paper_auc, improve_auc])\n",
        "    x = scikit_posthocs.posthoc_nemenyi_friedman(data.T)\n",
        "    print(x)\n",
        "\n",
        "\n",
        "def stat_test(excel_name):\n",
        "    baseline_auc, paper_auc, improve_auc, friedman = friedman_test(excel_name)\n",
        "    print(f'Friedman test p-value = {friedman.pvalue}')\n",
        "    post_hoc_test(baseline_auc, paper_auc, improve_auc)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mauaEMERYCd6",
        "outputId": "d17f671d-f18e-4217-faab-ba130c29f745"
      },
      "source": [
        "stat_test(path_drive+'Results/Results.xlsx')"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Friedman test p-value = 0.036787944117144245\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gea4bUFAcOUa"
      },
      "source": [
        "### Load the datasets to google drive\n",
        "#### Not need to run!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQ0xLsmrOOjK"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\"\"\"\n",
        "Data importer - for each dataset there is function called \"import_{dataset_name}\" \n",
        "These functions used tensorflow datasets to load datasets and save them as np array under the Datasets folder\n",
        "\"\"\"\n",
        "def data_load():\n",
        "    import_svhn()\n",
        "    import_stl()\n",
        "    import_beans()\n",
        "    import_casava()\n",
        "    import_cmater()\n",
        "    import_coloret()\n",
        "    import_oxford_f()\n",
        "    import_rps()\n",
        "    import_ct_birds()\n",
        "    import_cifar_100()\n",
        "\n",
        "\n",
        "def import_cifar_100():\n",
        "    dataset_cifar100 = tf.keras.datasets.cifar100.load_data()\n",
        "    full_x_cifar100 = np.concatenate((dataset_cifar100[0][0], dataset_cifar100[1][0]), axis=0)\n",
        "    full_y_cifar100 = np.concatenate((dataset_cifar100[0][1], dataset_cifar100[1][1]), axis=0)\n",
        "    x_full_1 = np.concatenate([full_x_cifar100[np.where(full_y_cifar100 == x)[0]] for x in range(0, 20)])\n",
        "    y_full_1 = np.concatenate([full_y_cifar100[np.where(full_y_cifar100 == x)[0]] for x in range(0, 20)])\n",
        "    y_full_1 = np.array(y_full_1 % 20)\n",
        "    x_full_2 = np.concatenate([full_x_cifar100[np.where(full_y_cifar100 == x)[0]] for x in range(20, 40)])\n",
        "    y_full_2 = np.concatenate([full_y_cifar100[np.where(full_y_cifar100 == x)[0]] for x in range(20, 40)])\n",
        "    y_full_2 = np.array(y_full_2 % 20)\n",
        "    x_full_3 = np.concatenate([full_x_cifar100[np.where(full_y_cifar100 == x)[0]] for x in range(40, 60)])\n",
        "    y_full_3 = np.concatenate([full_y_cifar100[np.where(full_y_cifar100 == x)[0]] for x in range(40, 60)])\n",
        "    y_full_3 = np.array(y_full_3 % 20)\n",
        "    x_full_4 = np.concatenate([full_x_cifar100[np.where(full_y_cifar100 == x)[0]] for x in range(60, 80)])\n",
        "    y_full_4 = np.concatenate([full_y_cifar100[np.where(full_y_cifar100 == x)[0]] for x in range(60, 80)])\n",
        "    y_full_4 = np.array(y_full_4 % 20)\n",
        "    x_full_5 = np.concatenate([full_x_cifar100[np.where(full_y_cifar100 == x)[0]] for x in range(80, 100)])\n",
        "    y_full_5 = np.concatenate([full_y_cifar100[np.where(full_y_cifar100 == x)[0]] for x in range(80, 100)])\n",
        "    y_full_5 = np.array(y_full_5 % 20)\n",
        "    np.save(f\"{datasets_path_drive}Cifar100_1_X.npy\", x_full_1)\n",
        "    np.save(f\"{datasets_path_drive}Cifar100_1_Y.npy\", y_full_1)\n",
        "    np.save(f\"{datasets_path_drive}Cifar100_2_X.npy\", x_full_2)\n",
        "    np.save(f\"{datasets_path_drive}Cifar100_2_Y.npy\", y_full_2)\n",
        "    np.save(f\"{datasets_path_drive}Cifar100_3_X.npy\", x_full_3)\n",
        "    np.save(f\"{datasets_path_drive}Cifar100_3_Y.npy\", y_full_3)\n",
        "    np.save(f\"{datasets_path_drive}Cifar100_4_X.npy\", x_full_4)\n",
        "    np.save(f\"{datasets_path_drive}Cifar100_4_Y.npy\", y_full_4)\n",
        "    np.save(f\"{datasets_path_drive}Cifar100_5_X.npy\", x_full_5)\n",
        "    np.save(f\"{datasets_path_drive}Cifar100_5_Y.npy\", y_full_5)\n",
        "\n",
        "\n",
        "def import_ct_birds():\n",
        "    ct_data = tfds.load('caltech_birds2010', split='train+test')\n",
        "    ct_data_np = tfds.as_numpy(ct_data)\n",
        "    birds_dataset = np.array([x for x in ct_data_np])\n",
        "    ctbirds_x = np.array([tf.image.resize(x['image'], (64, 64)).numpy() for x in birds_dataset])\n",
        "    ctbirds_y = np.array([x['label'] for x in birds_dataset])\n",
        "    ctbirds_y = ctbirds_y.reshape((ctbirds_y.shape[0], 1))\n",
        "    ctb_x_full_1 = np.concatenate([ctbirds_x[np.where(ctbirds_y == x)[0]] for x in range(0, 70)])\n",
        "    ctb_y_full_1 = np.concatenate([ctbirds_y[np.where(ctbirds_y == x)[0]] for x in range(0, 70)])\n",
        "    ctb_x_full_2 = np.concatenate([ctbirds_x[np.where(ctbirds_y == x)[0]] for x in range(70, 140)])\n",
        "    ctb_y_full_2 = np.concatenate([ctbirds_y[np.where(ctbirds_y == x)[0]] for x in range(70, 140)])\n",
        "    ctb_y_full_2 = np.array(ctb_y_full_2 % 70)\n",
        "    ctb_x_full_3 = np.concatenate([ctbirds_x[np.where(ctbirds_y == x)[0]] for x in range(140, 200)])\n",
        "    ctb_y_full_3 = np.concatenate([ctbirds_y[np.where(ctbirds_y == x)[0]] for x in range(140, 200)])\n",
        "    ctb_y_full_3 = np.array(ctb_y_full_3 % 70)\n",
        "    np.save(f\"{datasets_path_drive}Ctb_1_X.npy\", ctb_x_full_1)\n",
        "    np.save(f\"{datasets_path_drive}Ctb_1_Y.npy\", ctb_y_full_1)\n",
        "    np.save(f\"{datasets_path_drive}Ctb_2_X.npy\", ctb_x_full_2)\n",
        "    np.save(f\"{datasets_path_drive}Ctb_2_Y.npy\", ctb_y_full_2)\n",
        "    np.save(f\"{datasets_path_drive}Ctb_3_X.npy\", ctb_x_full_3)\n",
        "    np.save(f\"{datasets_path_drive}Ctb_3_Y.npy\", ctb_y_full_3)\n",
        "\n",
        "\n",
        "def import_rps():\n",
        "    rps_data = tfds.load('rock_paper_scissors', split='train+test')\n",
        "    rps_np = tfds.as_numpy(rps_data)\n",
        "    dataset_rps = np.array([x for x in rps_np])\n",
        "    rps_x = np.array([tf.image.resize(x['image'], (100, 100)).numpy() for x in dataset_rps])\n",
        "    rps_y = np.array([x['label'] for x in dataset_rps])\n",
        "    rps_y = rps_y.reshape((rps_y.shape[0], 1))\n",
        "    np.save(f\"{datasets_path_drive}Rps_X.npy\", rps_x)\n",
        "    np.save(f\"{datasets_path_drive}Rps_Y.npy\", rps_y)\n",
        "\n",
        "\n",
        "def import_oxford_f():\n",
        "    oxford_data = tfds.load('oxford_flowers102', split='train+test+validation')\n",
        "    oxford_np = tfds.as_numpy(oxford_data)\n",
        "    dataset_oxford = np.array([x for x in oxford_np])\n",
        "    oxford_x = np.array([tf.image.resize(x['image'], (64, 64)).numpy() for x in dataset_oxford])\n",
        "    oxford_y = np.array([x['label'] for x in dataset_oxford])\n",
        "    oxford_y = oxford_y.reshape((oxford_y.shape[0], 1))\n",
        "    oxford_x_full_1 = np.concatenate([oxford_x[np.where(oxford_y == x)[0]] for x in range(0, 51)])\n",
        "    oxford_y_full_1 = np.concatenate([oxford_y[np.where(oxford_y == x)[0]] for x in range(0, 51)])\n",
        "    oxford_x_full_2 = np.concatenate([oxford_x[np.where(oxford_y == x)[0]] for x in range(51, 102)])\n",
        "    oxford_y_full_2 = np.concatenate([oxford_y[np.where(oxford_y == x)[0]] for x in range(51, 102)])\n",
        "    oxford_y_full_2 = np.array(oxford_y_full_2 % 51)\n",
        "    np.save(f\"{datasets_path_drive}Oxford_1_X.npy\", oxford_x_full_1)\n",
        "    np.save(f\"{datasets_path_drive}Oxford_1_Y.npy\", oxford_y_full_1)\n",
        "    np.save(f\"{datasets_path_drive}Oxford_2_X.npy\", oxford_x_full_2)\n",
        "    np.save(f\"{datasets_path_drive}Oxford_2_Y.npy\", oxford_y_full_2)\n",
        "\n",
        "\n",
        "def import_coloret():\n",
        "    coloret_data = tfds.load('colorectal_histology', split='train')\n",
        "    coloret_np = tfds.as_numpy(coloret_data)\n",
        "    dataset_coloret = np.array([x for x in coloret_np])\n",
        "    coloret_x = np.array([x['image'] for x in dataset_coloret])\n",
        "    coloret_y = np.array([x['label'] for x in dataset_coloret])\n",
        "    coloret_y = coloret_y.reshape((coloret_y.shape[0], 1))\n",
        "    np.save(f\"{datasets_path_drive}Coloret_X.npy\", coloret_x)\n",
        "    np.save(f\"{datasets_path_drive}Coloret_Y.npy\", coloret_y)\n",
        "\n",
        "\n",
        "def import_cmater():\n",
        "    cmater_data = tfds.load('cmaterdb', split='train+test')\n",
        "    cmater_np = tfds.as_numpy(cmater_data)\n",
        "    dataset_cmater = np.array([x for x in cmater_np])\n",
        "    cmater_x = np.array([x['image'] for x in dataset_cmater])\n",
        "    cmater_y = np.array([x['label'] for x in dataset_cmater])\n",
        "    cmater_y = cmater_y.reshape((cmater_y.shape[0], 1))\n",
        "    np.save(f\"{datasets_path_drive}Cmater_X.npy\", cmater_x)\n",
        "    np.save(f\"{datasets_path_drive}Cmater_Y.npy\", cmater_y)\n",
        "\n",
        "\n",
        "def import_casava():\n",
        "    casave_data = tfds.load('cassava', split='train+test+validation')\n",
        "    casava_np = tfds.as_numpy(casave_data)\n",
        "    dataset_casava = np.array([x for x in casava_np])\n",
        "    casava_x = np.array([tf.image.resize(x['image'], (64, 64)).numpy() for x in dataset_casava])\n",
        "    casava_y = np.array([x['label'] for x in dataset_casava])\n",
        "    casava_y = casava_y.reshape((casava_y.shape[0], 1))\n",
        "    np.save(f\"{datasets_path_drive}Casava_X.npy\", casava_x)\n",
        "    np.save(f\"{datasets_path_drive}Casava_Y.npy\", casava_y)\n",
        "\n",
        "\n",
        "def import_beans():\n",
        "    data_test = tfds.load('beans', split='train+test+validation')\n",
        "    test_np = tfds.as_numpy(data_test)\n",
        "    dataset_beans = np.array([x for x in test_np])\n",
        "    beans_x = np.array([x['image'] for x in dataset_beans])\n",
        "    beans_y = np.array([x['label'] for x in dataset_beans])\n",
        "    beans_x = tf.image.resize(beans_x, (32, 32)).numpy()\n",
        "    beans_y = beans_y.reshape((beans_y.shape[0], 1))\n",
        "    np.save(f\"{datasets_path_drive}Beans_X.npy\", beans_x)\n",
        "    np.save(f\"{datasets_path_drive}Beans_Y.npy\", beans_y)\n",
        "\n",
        "\n",
        "def import_stl():\n",
        "    leaves_data = tfds.load('stl10', split='train+test')\n",
        "    leaves_np = tfds.as_numpy(leaves_data)\n",
        "    dataset_leaves = np.array([x for x in leaves_np])\n",
        "    leaves_x = np.array([x['image'] for x in dataset_leaves])\n",
        "    leaves_y = np.array([x['label'] for x in dataset_leaves])\n",
        "    leaves_y = leaves_y.reshape((leaves_y.shape[0], 1))\n",
        "    x_stl_1 = np.concatenate([leaves_x[np.where(leaves_y == x)[0]] for x in range(0, 4)])\n",
        "    y_stl_1 = np.concatenate([leaves_y[np.where(leaves_y == x)[0]] for x in range(0, 4)])\n",
        "    x_stl_2 = np.concatenate([leaves_x[np.where(leaves_y == x)[0]] for x in range(4, 8)])\n",
        "    y_stl_2 = np.concatenate([leaves_y[np.where(leaves_y == x)[0]] for x in range(4, 8)])\n",
        "    y_stl_2 = np.array(y_stl_2 % 4)\n",
        "    x_stl_3 = np.concatenate([leaves_x[np.where(leaves_y == x)[0]] for x in range(6, 10)])\n",
        "    y_stl_3 = np.concatenate([leaves_y[np.where(leaves_y == x)[0]] for x in range(6, 10)])\n",
        "    y_stl_3 = np.array(y_stl_3 % 4)\n",
        "    np.save(f\"{datasets_path_drive}stl_1_X.npy\", x_stl_1)\n",
        "    np.save(f\"{datasets_path_drive}stl_1_Y.npy\", y_stl_1)\n",
        "    np.save(f\"{datasets_path_drive}stl_2_X.npy\", x_stl_2)\n",
        "    np.save(f\"{datasets_path_drive}stl_2_Y.npy\", y_stl_2)\n",
        "    np.save(f\"{datasets_path_drive}stl_3_X.npy\", x_stl_3)\n",
        "    np.save(f\"{datasets_path_drive}stl_3_Y.npy\", y_stl_3)\n",
        "\n",
        "\n",
        "def import_svhn():\n",
        "    svhn_data = tfds.load('svhn_cropped', split='train+test')\n",
        "    svhn_np = tfds.as_numpy(svhn_data)\n",
        "    dataset_svhn = np.array([x for x in svhn_np])\n",
        "    svhn_x = np.array([x['image'] for x in dataset_svhn])\n",
        "    svhn_y = np.array([x['label'] for x in dataset_svhn])\n",
        "    svhn_y = svhn_y.reshape((svhn_y.shape[0], 1))\n",
        "    x_shvn_1 = np.concatenate([svhn_x[np.where(svhn_y == x)[0]] for x in range(0, 4)])\n",
        "    y_shvn_1 = np.concatenate([svhn_y[np.where(svhn_y == x)[0]] for x in range(0, 4)])\n",
        "    x_shvn_2 = np.concatenate([svhn_x[np.where(svhn_y == x)[0]] for x in range(4, 8)])\n",
        "    y_shvn_2 = np.concatenate([svhn_y[np.where(svhn_y == x)[0]] for x in range(4, 8)])\n",
        "    y_shvn_2 = np.array(y_shvn_2 % 4)\n",
        "    x_shvn_3 = np.concatenate([svhn_x[np.where(svhn_y == x)[0]] for x in range(6, 10)])\n",
        "    y_shvn_3 = np.concatenate([svhn_y[np.where(svhn_y == x)[0]] for x in range(6, 10)])\n",
        "    y_shvn_3 = np.array(y_shvn_3 % 4)\n",
        "    np.save(f\"{datasets_path_drive}svhn_1_X.npy\", x_shvn_1)\n",
        "    np.save(f\"{datasets_path_drive}svhn_1_Y.npy\", y_shvn_1)\n",
        "    np.save(f\"{datasets_path_drive}svhn_2_X.npy\", x_shvn_2)\n",
        "    np.save(f\"{datasets_path_drive}svhn_2_Y.npy\", y_shvn_2)\n",
        "    np.save(f\"{datasets_path_drive}svhn_3_X.npy\", x_shvn_3)\n",
        "    np.save(f\"{datasets_path_drive}svhn_3_Y.npy\", y_shvn_3)\n",
        "  \n",
        "data_load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuv09c_eCYNp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}